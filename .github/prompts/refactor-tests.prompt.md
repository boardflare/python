## Refactor Function to Data-Driven Testing Format

Goal: Convert the Python function and its tests in the provided folder to use a data-driven approach with `pytest` and a `test_cases.json` file.

Context: You will be given a folder containing:
*   `{function_name}.py`: The Python function implementation.
*   `test_{function_name}.py`: The existing pytest test file.
*   `{function_name}.md`: Documentation file.

Instructions for Test Case Descriptions:
- All test case descriptions in `test_cases.json` should be written from the perspective of an Excel user. Use Excel terminology (e.g., refer to 2D lists as "ranges" instead of "2D lists"). Describe scenarios as an Excel user would encounter them in the Excel interface.

Steps:

1.  **Start with the documentation file**:
    *   Begin by examining `{function_name}.md` to identify all examples shown in the documentation.
    *   These examples must be your first priority and will form the basis for all test cases marked with `"demo": true`.
    *   For each example in the documentation, note:
        * The exact input arguments used (prompt, data ranges, optional parameters)
        * The example context and business scenario
        * The expected output format

2.  **Then analyze the existing test file**:
    *   After extracting all documentation examples, examine `test_{function_name}.py` to identify:
        * Any additional test scenarios not covered by documentation examples
        * Specific assertions made beyond basic type/existence checks
        * Edge cases and error handling tests
    *   These will form the basis for test cases marked with `"demo": false`.

3.  Create `test_cases.json`:
    *   In the same folder, create a new file named `test_cases.json`.
    *   Structure the file with a top-level key `"test_cases"` containing a JSON list `[]`.
    *   **First add the documentation examples**:
        *   Add a JSON object for each example in the documentation file with:
            *   `"id"`: A unique string identifier matching the example (e.g., `"test_hr_engagement_summary"`).
            *   `"description"`: Use the exact scenario description from the documentation.
            *   `"arguments"`: Include all arguments exactly as shown in the documentation.
            *   `"demo": true`: Mark all documentation examples as demo cases.
            *   `"expected_rows"`: Estimate how many rows the output will need.
            *   (Optional) `"expected_contains_any"`: Include relevant keywords for validation if needed.
        *   The order of these test cases should match the order of examples in the documentation.
    *   **Then add test-only cases**:
        *   For each additional test scenario identified in the test file but not in documentation:
            *   `"id"`: A unique string identifier for the test case.
            *   `"description"`: A brief description from an Excel user's perspective.
            *   `"arguments"`: The arguments needed for this test case.
            *   `"demo": false`: Mark as non-demo cases.
            *   Add any relevant validation keys (`expected_contains`, etc.).

4.  Refactor `test_{function_name}.py`:
    *   Keep necessary imports (`pytest`, `json`, `pathlib`, the function itself).
    *   Remove all the original individual `test_...` functions.
    *   Add a helper function `load_test_cases()`, which should:
        *   Use `pathlib` to locate and open `test_cases.json`.
        *   Parse the JSON and extract the list from the `"test_cases"` key.
        *   Iterate through this list, wrapping each test case dictionary in `pytest.param(case, id=case.get("id"))`.
        *   Return the list of `pytest.param` objects.
    *   Add a single parameterized test function, e.g., `test_{function_name}_parametrized(test_case)`, decorated with `@pytest.mark.parametrize("test_case", load_test_cases())`:
        *   Extract the `arguments` dictionary from the `test_case` parameter.
        *   Call the `{function_name}` function using `**arguments`.
        *   Implement basic assertions (e.g., `assert isinstance(result, expected_type)`, `assert result is not None`). 
        *   Implement conditional assertions based on the presence of validation keys in the `test_case` dictionary.

5.  Verification:
    *   Run the refactored tests using the command:
        ```powershell
        python -m pytest path\to\folder\test_{function_name}.py
        ```
    *   Ensure all tests pass.
    *   Run the build script to verify documentation alignment:
        ```powershell
        python examples\build_examples.py
        ```
    *   **Critical check**: Verify that the examples generated by the build script exactly match the examples in the documentation file. The demo test cases must be identical to the documentation examples.

Reference Examples:
See the following files for examples of the expected finished product:
- [ai_ask.py](../../examples/files/text/ai_ask/ai_ask.py)
- [test_ai_ask.py](../../examples/files/text/ai_ask/test_ai_ask.py)
- [test_cases.json](../../examples/files/text/ai_ask/test_cases.json)

Deliverable: The updated `test_{function_name}.py` file and the new `test_cases.json` file for the provided function folder.