<!-- gradio_lite_ai_ask.html: Gradio Lite web page for the ai_ask demo using file tags -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>AI Ask Gradio Lite Demo</title>
    <script type="module" crossorigin src="https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.css">
    <style>
        body { font-family: Arial, sans-serif; margin: 0; padding: 0; background: #f7f7f7; }
        .container { width: 800px; max-width: none; margin: 0 40px; background: #fff; border-radius: 0; box-shadow: none; padding: 32px 0; }
    </style>
</head>
<body>
<div class="container">
    <h1>AI Ask Gradio Lite Demo</h1>
    <p>This page runs the <b>ai_ask</b> Gradio app in your browser using Gradio Lite and Pyodide. No server required!</p>
    <gradio-lite>
        <gradio-file name="gradio_ai_ask.py" entrypoint>
# Gradio demo for ai_ask
import gradio as gr
import pandas as pd
from ai_ask import ai_ask
import json
import os

def run_ai_ask(prompt, data, temperature, max_tokens, model):
    data_list = data.values.tolist() if data is not None and not data.empty else None
    return ai_ask(prompt, data=data_list, temperature=temperature, max_tokens=max_tokens, model=model)

def load_demo_examples():
    test_cases_path = os.path.join(os.path.dirname(__file__), "test_cases.json")
    with open(test_cases_path, "r", encoding="utf-8") as f:
        cases = json.load(f)
    examples = []
    for case in cases:
        if case.get("demo"):
            args = case["arguments"]
            prompt = args.get("prompt", "")
            data = args.get("data", None)
            temperature = args.get("temperature", 0.5)
            max_tokens = args.get("max_tokens", 250)
            model = args.get("model", "mistral-small-latest")
            data_df = pd.DataFrame(data) if data is not None else None
            examples.append([
                prompt,
                data_df,
                temperature,
                max_tokens,
                model
            ])
    return examples

examples = load_demo_examples()

with gr.Blocks() as demo:
    gr.Markdown("# AI Ask Demo\nEnter a prompt and optional data for analysis.")
    with gr.Row():
        prompt = gr.Textbox(label="Prompt", lines=2)
    with gr.Row():
        data = gr.Dataframe(label="Data (optional)", headers=None, datatype="str", row_count=(1, "dynamic"), col_count=(1, "dynamic"))
    with gr.Row():
        temperature = gr.Slider(0.0, 2.0, value=0.5, step=0.01, label="Temperature")
        max_tokens = gr.Number(value=250, label="Max Tokens")
        model = gr.Textbox(value="mistral-small-latest", label="Model")
    output = gr.Textbox(label="AI Response")
    submit = gr.Button("Run")
    submit.click(run_ai_ask, inputs=[prompt, data, temperature, max_tokens, model], outputs=output)
    gr.Examples(
        examples=examples,
        inputs=[prompt, data, temperature, max_tokens, model],
        outputs=output,
        label="Demo Examples"
    )

demo.launch()
        </gradio-file>
        <gradio-file name="ai_ask.py">
import requests
import json

def ai_ask(prompt, data=None, temperature=0.5, max_tokens=250, model='mistral-small-latest'):
    """
    Uses AI to generate responses based on prompts and optional data ranges.

    Args:
        prompt (str): The question, task, or analysis to perform
        data (list, optional): 2D list containing data from Excel range to analyze
        temperature (float, optional): Controls response creativity (0-2). Default is 0.5
        max_tokens (int, optional): Maximum tokens for response generation
        model (str, optional): ID of the model to use
        # Note: API key is hardcoded for this example, replace with secure handling in production

    Returns:
        str: The AI-generated response
    """
    
    # Using Boardflare API for demo purposes. Replace with any OpenAI compatible API endpoint.
    # Sign up for your free Mistral API account at https://console.mistral.ai/ then replace the following:
    api_url = "https://llm.boardflare.com" # replace with "https://api.mistral.ai/v1/chat/completions"
    api_key = "cV4a59t1wjYGs...." # replace with your Mistral API key
    
    # Construct the message incorporating both prompt and data if provided
    message = prompt
    if data is not None:
        data_str = json.dumps(data, indent=2)
        message += f"\n\nData to analyze:\n{data_str}"
    
    # Prepare the API request payload
    payload = {
        "messages": [{"role": "user", "content": message}],
        "temperature": temperature,
        "model": model,
        "max_tokens": max_tokens
    }
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "Accept": "application/json"
    }
    
    # Make the API request
    response = requests.post(api_url, headers=headers, json=payload)
    response.raise_for_status()
    
    # Extract and return the response content
    response_data = response.json()
    content = response_data["choices"][0]["message"]["content"]
    return content
        </gradio-file>
        <gradio-file name="test_cases.json">
[
    {"id": "test_hr_engagement_summary", "description": "HR: Summarize employee engagement survey results.", "arguments": {"prompt": "Summarize the key findings from the employee engagement survey in 1 sentence:", "data": [["Question", "Score"], ["Team collaboration", 4.5], ["Workload", 3.2], ["Career advancement", 3.0], ["Management support", 4.0]]}, "expected_contains_any": ["collaboration", "workload", "career"], "expected_rows": 2, "demo": true},
    {"id": "test_sales_quarterly_analysis", "description": "Sales: Analyze quarterly sales data and provide insights.", "arguments": {"prompt": "Provide a brief analysis of the quarterly sales performance in 1 sentence:", "data": [["Region", "Q1", "Q2", "Q3", "Q4"], ["North", 120, 135, 150, 160], ["South", 100, 110, 120, 130], ["Central", 90, 95, 100, 105]]}, "expected_contains_any": ["North", "growth", "sales"], "expected_rows": 2, "demo": true},
    {"id": "test_operations_incident_summary", "description": "Operations: Summarize an incident report.", "arguments": {"prompt": "Summarize the following incident report in 1 sentence:", "data": [["On April 10th, a system outage affected order processing for 2 hours. The IT team resolved the issue by updating server configurations. No data loss occurred."]]}, "expected_contains_any": ["outage", "resolved", "data loss"], "expected_rows": 2, "demo": true},
    {"id": "test_business_followup_email", "description": "Business Writing: Draft a customer follow-up email.", "arguments": {"prompt": "Draft a follow-up email to a client after a successful product demo. Limit to 1 email."}, "expected_contains_any_lower": ["thank you", "product", "next steps"], "expected_rows": 2, "demo": true}
]
        </gradio-file>
        <gradio-file name="requirements.txt">
gradio
pandas
requests
        </gradio-file>
    </gradio-lite>
    <p style="margin-top:2em;font-size:0.95em;color:#666;">Powered by <a href="https://www.gradio.app/guides/gradio-lite" target="_blank">Gradio Lite</a> and Pyodide. All files are embedded in this HTML.</p>
</div>
</body>
</html>
